Polymind Research: Lightweight Meaning Atom Schema & Conceptual Blending

Defining a Lightweight Concept/Relationship Schema (“Meaning Atoms”)

Minimal Upper Ontologies and Schema Patterns: Many knowledge representation systems use a small, general set of relationship types to capture semantic connections. Foundational or upper ontologies like SUMO or DOLCE define very general relations (e.g. subclass-of, part-of, attribute-of) that apply across domains. For practical purposes, knowledge graphs often adopt a lightweight schema with a limited vocabulary of relations – for example, the ConceptNet common-sense graph uses a closed set of about 34 relation types (such as IsA, PartOf, HasProperty, UsedFor, CapableOf, Causes, etc.) ￼ ￼. These represent broad semantic roles independent of specific wording (e.g. A HasProperty B covers “A is B (as an attribute)”). Likewise, schema.org defines many domain-specific properties, but the idea for Polymind is to identify a minimal set of high-level relations (like the examples given: IS_A (taxonomic/category), PART_OF (mereology/composition), CAUSES (cause-effect), ENABLES (enables/precondition), HAS_PROPERTY (attributes), etc.) that can describe most relationships in a normalized way. Using such a schema makes the knowledge graph easier to query and avoids an explosion of one-off relation phrases.

Controlled Relationship Types in Knowledge Graphs: Adopting a limited ontology of relation types means that free-text relationship phrases extracted from language should be mapped to the closest canonical relation. For instance, if a sentence says “X is a kind of Y” or “X is a Y,” that should be normalized to an IS_A relation. “X has Y” (where Y is a quality) could map to HAS_PROPERTY. If the text says “X causes Y” or “X leads to Y,” normalize to CAUSES, and similarly map “X allows Y” or “X helps bring about Y” to ENABLES, etc. The key is to design a mapping schema that covers common semantic patterns. This can draw from existing ontologies – for example, WordNet and ConceptNet both include relations for taxonomic, meronymic, causal, and attributive links. It’s often useful to have a generic fallback like RELATED_TO for uncategorized links ￼, but the goal is to use it sparingly by covering as many meaningful relations as possible with the defined set.

Prompting LLMs to Classify Relations into Types: A large language model (such as Gemini) can be guided via its prompt to output relationships using the controlled vocabulary rather than arbitrary phrasing. Best practices from recent work suggest explicitly instructing the model on the allowed relation types. For example, instead of asking the LLM “extract subject-predicate-object triples,” one can prompt: “Extract facts as triples (Subject – Relation – Object) using only the following relationship types: IsA, PartOf, HasProperty, Causes, Enables, etc. (use whichever fits best).” By enumerating the desired relations (and even giving a brief description or parenthetical explanation of each), the model is more likely to choose one of them if it can ￼. Research by GraphAware demonstrated that a generic extraction prompt often yields very fine-grained or verbose relations (e.g. “was a central and willing player” as a relation) which would lead to a bloated schema. By limiting the relation set and “forcing” standard types, the extractions became much more consistent ￼. In their example, they replaced free-form relations with a set of standard ones like works for or located in, and the model’s output aligned with those classes ￼. The prompt can list the relations and even clarify tricky ones (GraphAware found that providing definitions or rephrasing relation names improved the model’s understanding ￼). It’s also helpful to show a few-shot example: e.g., provide a sample sentence and a correctly formatted triple with a normalized relation. This “one-shot” or few-shot prompt acts as a demonstration of the expected classification, boosting accuracy ￼.

Improving Semantic Consistency in Extraction: Ensuring the model sticks to the schema requires careful prompt design and sometimes post-processing. Techniques to improve consistency include: (1) Encouraging brevity and clarity – instruct the LLM to use short relation phrases (1-3 words) and avoid pronouns or overly specific wording ￼. This makes it easier to map to a controlled vocabulary (e.g., “located in” instead of “was traditionally situated in”). (2) If the LLM occasionally produces a relation outside the allowed set, a post-processing step can attempt to map synonyms or variants to the nearest schema relation. For example, if it outputs “X is part of Y” or “X belongs to Y,” those phrases can be recognized (via simple string matching or a small dictionary) as equivalents of PART_OF and unified accordingly. In more ambiguous cases, one could use an LLM in a secondary pass: “The relation ‘X feeds Y’ was extracted – which standard relation (IsA, PartOf, Causes, etc.) does this best correspond to?” and use its answer to map to, say, CAUSES (if the context implies causation) or another suitable type. (3) Validate by type constraints: If the knowledge graph ontology defines domain and range for relations (for instance, CAUSES should link events or processes, IS_A should link an instance to a class or subclass to superclass), then implausible triples can be flagged. While a GPT-based extractor might not always respect ontological constraints, you can have a rule-based filter or ask the LLM to double-check: e.g., “Ensure that the subject and object make sense for the relation type (e.g., if using PartOf, the object should be a larger whole).” This kind of instruction can be baked into the prompt to improve semantic validity.

Adapting extract_meaning_atoms_gemini: The extract_meaning_atoms_gemini function can be modified to incorporate the above prompt strategies. For example, when constructing the prompt for Gemini, you can add a section that defines the relationship schema. A possible adaptation:
	•	Begin the prompt with a brief overview of the ontology, e.g., “We represent knowledge in triples: (Subject, RelationType, Object). Use these RelationType categories whenever possible: IS_A (A is a type or instance of B), PART_OF (A is a component or member of B), HAS_PROPERTY (A has attribute/property B), CAUSES (A brings about/effects B), ENABLES (A makes B possible), RELATED_TO (catch-all for other relations).”
	•	Then provide the text or chunk to analyze and ask for output like: “Extract all factual triples from the text, mapping them to the above relation types. Only use the provided RelationType labels (in uppercase) for the predicate.” Possibly demonstrate one example if the format is complex.

This way, Gemini’s output for each triple should contain a predicate from the controlled list (or a default like RELATED_TO if nothing fits). It’s important to still allow some flexibility (the model might not find an exact fit every time), but by emphasizing the limited options, we encourage mappable outputs. In practice, testing and iteration are needed: if Gemini tends to ignore the instruction occasionally, you might reinforce it by making the prompt more explicit (or using all-caps for the relation names, etc.). The GraphAware experience suggests that being very explicit and even redundant in prompt instructions can help steer the model ￼ ￼. Also, consider splitting the task: first have the model extract a raw triple (or identify the relation in free words), and then in a second step ask it to classify that relation into one of the ontology types. This two-step approach (extract then classify) can improve accuracy if direct classification is challenging, though it’s slower.

Mapping to Ontology in Neo4j Ingestion: On the Neo4j side, the load_triples_to_neo4j process can enforce the schema by mapping any incoming triple’s relation text to an actual Neo4j relationship type. In Neo4j, relationship types are typically fixed designations (like IS_A or PART_OF) rather than arbitrary strings. A simple implementation is to maintain a mapping dictionary in the code. For example, {"is a": "IS_A", "is an example of": "IS_A", "part of": "PART_OF", "located in": "PART_OF", "causes": "CAUSES", "leads to": "CAUSES", "enables": "ENABLES", "allows": "ENABLES", "has": "HAS_PROPERTY", "has a": "HAS_PROPERTY", ...} and so on. Before creating a relationship in Neo4j, the code can normalize the predicate string by lowercasing and removing punctuation, then look up the closest match in this map. If found, it uses the standardized relationship type (and you might store the original phrase as a property on the relationship for reference if needed). If not found, you could default to a generic RELATED_TO type or create a custom type on the fly. However, since the goal is a controlled vocabulary, it’s better to funnel as much as possible into the predefined types. Leveraging Neo4j’s native typing means queries can be more precise (e.g., MATCH (a)-[:CAUSES]->(b) will retrieve all causal relations easily).

Another approach is to incorporate the ontology mapping during ingestion using Cypher or APOC procedures. For instance, you might have a small reference node that holds synonyms or use APOC text functions to decide the relation type. But a straightforward method is to handle it in Python before writing to Neo4j. Given that extract_meaning_atoms_gemini will already be trying to output the proper types, the ingestion step becomes simpler: it can trust that most relations are already one of the allowed ones (like “IS_A”) and directly create [:IS_A] relationships in Neo4j. For any that are not, a quick classification (either via code or a secondary LLM call) can resolve it.

By integrating this mapping, Polymind’s graph will have clean, standardized edge types. This enables using Neo4j’s features like optimized indexes on relationship types and the ability to add constraints or styling per type in visualization. It also aligns with the idea of an ontology-backed knowledge graph, where the edges carry semantic meaning from a fixed set rather than being arbitrary text.

Example: Suppose the raw extraction gave: (Mars, is the fourth planet from, Sun). We’d want to map “is the fourth planet from” to something like IS_A (since it’s essentially a class membership: Mars is a planet, part of the category of planets) or perhaps PART_OF (if we interpret it as part of the solar system). A well-designed prompt might have avoided this complex phrase in the first place, but if not, our mapping layer can decide: contains “planet” -> likely an IsA relation, etc. The result would be a Neo4j edge (:CelestialBody "Mars") -[:IS_A]-> (:CelestialBody "Planet"). In contrast, a sentence like “Carbon dioxide causes global warming” would map “causes” directly to a CAUSES relationship, (:Chemical "Carbon Dioxide") -[:CAUSES]-> (:Phenomenon "Global Warming"). By enforcing these types, any future queries or analysis (like finding all cause-effect chains) becomes much easier.

Basic Conceptual Blending and Analogy Generation

Retrieving Features of Two Concepts: Conceptual blending requires us to understand the key characteristics of the two input concepts we want to combine. In Polymind’s context, this means pulling information from the knowledge graph about each concept. Practically, one can query Neo4j for each concept’s immediate neighbors and relationships – essentially its feature set in the graph. For example, if concept A is a node, we retrieve triples like (A) -[relation]-> (Neighbor) and (Other) -[relation]-> (A) to gather what A is, what parts it has, what it does, properties, etc. We might also retrieve any textual description stored for that node (if the graph has a description property or we can generate one by summarizing its relationships). The idea is to assemble a profile for Concept A and Concept B. For instance, if A = “Eagle” and B = “Ship”, we might find features: Eagle is_a Bird, has_property “wings”, capable_of “fly”, etc.; Ship is_a Vehicle, has_property “hull”, used_for “transport on water”, etc. We’d select the most salient facts (immediate neighbors usually suffice – things two hops away are often too general). This set of features will form the building blocks for blending.

It’s often useful to include any definitional or unique traits. If the graph or an external source provides a short description (e.g., from Wikipedia), that can be used to give context beyond the raw triples. In absence of a pre-written description, you can let the LLM infer from the triples by asking it to summarize each concept in a sentence. However, since the goal is to use the KG content, sticking to the actual relationships ensures the blend considers what’s known in our system about those concepts.

Prompting the LLM to Blend Concepts: Once we have the key attributes of Concept A and Concept B, we craft a prompt for the LLM (Gemini) that explains the two concepts and asks for a creative combination. A typical prompt could look like:

“We have two concepts: Concept A (description/traits…) and Concept B (description/traits…). Imagine a new concept that blends A and B, combining their features into a single entity. Describe this new concept, giving it a name and explaining its key features and capabilities drawn from both A and B.”

The prompt should encourage the model to not just list features from A and B, but to synthesize them – essentially creating a concept in the “blended space.” One approach is to explicitly request certain outputs: for example, ask for a name and a list of features. For instance:
	•	“Give the blended concept a creative compound name that reflects both inputs.”
	•	“List the new concept’s characteristics: what it inherits or extends from A, what from B, and any novel ability or purpose that emerges.”

By structuring the request (possibly as an outline or bullet points), we make it easier to parse later. For example, the model’s answer could be in a format:

**New Concept:** *Name* – A one-line tagline.

- **Origin:** A blend of [Concept A] and [Concept B].
- **Characteristics:** … (list of properties/abilities)
- **Uses/Applications:** … (how it could be used, if relevant)

This is just one format; even a short paragraph describing the concept can work, but lists make extraction of facts simpler. Importantly, the prompt should inspire imagination – use words like “imagine”, “creative”, “novel” – so the model knows it’s allowed to go beyond strict reality. Since LLMs are trained on a lot of fantastical or creative descriptions as well, they can produce surprisingly coherent blends.

Eliciting Emergent Properties: A hallmark of conceptual blending is that the combination can yield emergent features – properties not present in either concept in isolation, but arising from their combination. To get the model to produce these, the prompt can include an explicit nudge: “Describe not only how it has A’s and B’s features, but what new ability or benefit comes from combining them.” For example, blending “eagle” and “ship” might yield a concept of a “Skyship” – a large flying vessel with wings like an eagle and the carrying capacity of a ship. An emergent property could be long-range airborne cargo transport, which neither eagles (small, biological) nor ordinary ships (non-flying) alone have. By asking for “capabilities” or “the unique advantage of this hybrid”, we push the model to consider more than a surface-level mix. Another technique is to ask for a metaphor or problem it solves: “What problem does this blended concept solve, or what niche does it fill that neither original concept could by itself?” This often brings out creative angles (e.g., houseboat solves the need for a home that can travel on water).

In practice, LLMs are quite adept at analogy and blending because they’ve seen many examples of figurative language and “X meets Y” ideas. Prompting for emergent behavior leverages this. The model might produce some whimsical or impractical ideas – that’s okay in brainstorming mode, and Polymind can later filter or refine as needed. If more control is desired, one could provide an example of a simple blend to guide it. For instance: “Example: Blend ‘phone’ and ‘watch’ -> produced the concept of a ‘smartwatch’ which has the communication ability of a phone and the convenience of a wristwatch, enabling new features like fitness tracking on the go.” A one-shot example like this in the prompt illustrates the task clearly.

Parsing and Representing the Blended Concept: After the LLM outputs the description of the blended concept, we need to integrate this new concept into our knowledge graph. This involves a few steps:
	•	Identify the new concept’s name (the model hopefully provides one). Create a new node in Neo4j for this concept. It might be useful to tag it as a “BlendedConcept” (using a label or property) so we know it was AI-generated.
	•	Link it to the source concepts: A reasonable approach is to add relationships from the new node to the original Concept A and Concept B to capture the lineage. We might define a relation type like BLEND_OF or use something like IS_A twice (though “is a A” and “is a B” can be logically tricky if taken strictly, it could be seen as the new concept is a subclass of both – multiple inheritance). Using a custom relation (e.g., (:New) -[:BLEND_OF]-> (:A) and (:New) -[:BLEND_OF]-> (:B)) explicitly encodes that it was created from those two. This is useful for traceability and also for potentially retrieving all blends that involve a given concept.
	•	Extract features of the blended concept: We should add the key features described by the LLM as relationships in the graph. This can be done by running our extraction function on the LLM’s description (itself a piece of text). For example, if the output text says “The Skyship can soar through the air like an eagle and carry heavy cargo like a ship,” our extract_meaning_atoms_gemini (with its improved ontology prompt) might output triples like: (Skyship) IS_A Aircraft, (Skyship) HAS_PROPERTY "wingspan of 30 meters", (Skyship) CAPABLE_OF carry cargo, etc., or even link back to the original concepts’ features: (Skyship) CAPABLE_OF flight and (Skyship) CAPABLE_OF naval_transport (depending on how the text is phrased). We can then merge those into the graph. If some features reference existing concepts (e.g., “sky” or “water” or generic ideas), we might link to those nodes if they exist, or create new ones as needed. We may also want to capture any emergent feature explicitly – if the model gave something like “this combination enables long-distance aerial freight”, we could add a triple like (Skyship) ENABLES "long-distance airborne freight transport" or create a node for that concept.
	•	Structuring the output for parsing: If we anticipated the output format (say we asked for a bullet list of characteristics), parsing is straightforward (we can read each bullet as a potential triple or at least as a sentence to feed to the extractor). If the output is a freeform paragraph, we rely on the same triple extraction pipeline. In both cases, it’s essentially using the same machinery we used for the original text, but now on the model-generated content. We should be a bit cautious – LLM output might be more flowery or use analogies (“has the strength of an ox”) which our extractor might misinterpret. So it can help to prompt the LLM to be somewhat factual in the description of features (or even ask it to output some pseudo-triples itself, though that might reduce creativity). Another strategy: explicitly ask the LLM after it describes the concept to “list 3 concise facts about [NewConcept] that we could add to our knowledge graph.” This meta-prompt could yield three simple sentences that are easy to convert into triples, ensuring we capture the essence.

After adding the blended concept and its relations, we will have new nodes and edges enriching the graph. This can be iterative – one could even blend the blended concepts further, though that might get very abstract!

Example: If A = “Eagle” and B = “Ship”, the prompt might yield a concept named “Skyship” or “Eagleship.” The description might say: “The Skyship is a hybrid of a majestic eagle and an ocean ship. It has great wings and feathers like an eagle, allowing it to fly, and a large hull like a ship, enabling it to carry cargo and passengers. The Skyship can soar through the sky carrying heavy loads across long distances. In essence, it combines the navigation and capacity of a ship with the aerial freedom of a bird, creating a new transportation mode.” From this, we would create a node Skyship. We might add BLEND_OF links to Eagle and Ship. We’d extract facts such as Skyship has wings, has hull, capable_of flight, capable_of carrying heavy loads, used_for transport across distances, etc. Many of these properties mirror those of its parents (Eagle -> flight, Ship -> carrying cargo), but now unified in one entity. The emergent idea here is “long-distance aerial cargo transport”, which we could capture as well (perhaps as a node “AerialCargoTransport” that Skyship enables or is an instance of). This representation allows Polymind to later query or even visualize how the new concept inherits relations from both origins.

Existing Frameworks and Algorithms for Blending: The idea of blending concepts has been studied in computational creativity. Classical conceptual blending frameworks (like the work of Fauconnier & Turner’s theory, or implementations such as the Divago system by Pereira) often involve finding a “generic space” (common structure between concepts) and systematically mapping roles from each input into a “blended space.” These systems represent knowledge in structured form (ontologies, frames, or logic) and use search algorithms to find compatible combinations. For example, a formal blending algorithm might take an ontology graph of “house” and “boat”, find that both have a notion of “shelter” or “habitation” (house provides habitat on land, boat provides shelter at sea) as a common aspect, and then merge structures to create “houseboat.” These approaches can ensure logical consistency of the blend and have been used to generate novel ideas in a constrained manner. However, they require a well-defined knowledge base and can be computationally heavy (the search space is large and “most combinations are not meaningful” as noted in blending research) ￼. They also need careful encoding of what elements can map to what (which is domain-specific).

In contrast, using an LLM is a more flexible, data-driven approach: the LLM implicitly contains a lot of world knowledge and patterns of analogies, metaphors, etc. It can often produce a plausible blend without an explicit algorithmic mapping – essentially performing a kind of neural conceptual blending. The PopBlends system (CHI 2023) is an example where researchers combined LLMs with knowledge bases for blending pop culture concepts. They found that having the LLM generate associations and then guiding it with a connecting concept led to more creative outputs ￼ ￼. One insight from such work is that sometimes finding an intermediate linking concept improves blends – e.g., to blend A and B, find a concept C that relates to both (this is analogous to finding a generic space). In Polymind, one could implement a step to search the graph for common neighbors or categories of A and B. If, say, both A and B are types of “Vehicle”, the prompt could highlight that: “both are vehicles,” to focus the blend on that aspect. If no obvious commonality, the LLM might invent one, but giving it a hint (like “both share a theme of speed” or “one is known for X, the other for Y, imagine something that has both”) can guide creativity.

Another related task is analogy generation. Analogies differ from blends in that they usually map one domain to another (rather than fusing them). However, they share the need to recognize relationships. With a knowledge graph, one could attempt analogies by finding structural parallels: e.g., find two pairs of nodes that share the same relation, then ask the LLM to phrase “A is to B as C is to D”. Polymind could use its ontology for this – for instance, search for different instances of the same relationship. If we see pattern “A is the capital_of B” and “C is the capital_of D”, that yields an analogy “A is to B as C is to D” (e.g., Paris is to France as Tokyo is to Japan). For more creative analogies, one might use embeddings or LLM to identify loose similarities (“the relationship of a coach to a team might be analogously like that of a conductor to an orchestra”). While analogy generation wasn’t the main focus of Polymind’s question, it’s worth noting that analogical reasoning can complement blending: an analogy can highlight a shared structure that could then be the basis of a conceptual blend or a metaphor. In cognitive terms, blending is considered a form of combinational creativity, related to analogies and metaphors ￼.

In summary, existing frameworks (like formal conceptual blending algorithms or structure-mapping for analogies) provide structured ways to generate new concepts but can be complex to implement. For practical development, leveraging the LLM with prompt engineering is an incremental way to get results. One can start by manually selecting features from the graph and asking the LLM for a blend (as described), and later automate the feature retrieval and result parsing. Libraries or tools that might help include:
	•	Neo4j Graph Data Science or custom Cypher queries: for finding common properties or paths between two concepts (which could inspire a blend or analogy).
	•	ConceptNet or WordNet APIs: for getting common associations or hypernyms that could serve as the “generic space” or to validate that an emergent property makes sense.
	•	If needed, one could incorporate an existing analogical reasoning module (e.g., the Structure Mapping Engine from cognitive science, or newer ML models for analogy) to find good pairings to feed into blends. But these are optional – often the LLM itself, given the right prompt, will perform a kind of internal analogy finding.

Tool/Library Recommendations: In implementing this in polymind_app.py, you can do it step by step:
	•	Use the Neo4j Python driver to query neighbors of the concept nodes. This will give you the data to feed into the LLM prompt.
	•	Use the LLM (Gemini) via whatever API or library (OpenAI API or otherwise) you have, with carefully constructed prompts as discussed. Python libraries like openai or langchain could help manage prompts and responses if needed.
	•	To parse LLM output, you could continue using the same extraction function (maybe slightly tuned for shorter text). If you anticipate a structured output (like JSON or bullet lists), Python’s json or simple string parsing will do. Otherwise, feeding it back into extract_meaning_atoms_gemini is a good reuse.
	•	Finally, use the Neo4j driver again to write the new node and relationships (ensuring to use the standardized relation types for any new edges as well).

By incrementally building this pipeline – first manually testing a couple of blends with hardcoded prompts, then automating retrieval and insertion – you’ll add a concept invention capability to Polymind. This can dramatically expand the graph in novel directions, but thanks to the lightweight schema and controlled vocab, the new additions will remain interoperable with the rest of the knowledge graph.